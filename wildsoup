#!/usr/bin/env python

import os; import sys;

""" import rquiered modules"""
try:
	import json
	import argparse
	import requests
	from BeautifulSoup import BeautifulSoup
except ImportError as ie:
	print(str(ie))
	print "To install a module: pip install moduleName"
	sys.exit(-1)

def cmdArgs():
	parser = argparse.ArgumentParser()
	parser.add_argument('-r',required=True,help='Subreddit name')
	parser.add_argument('-u',required=True,help='Reddit username')
	parser.add_argument('-p',required=True,help='Reddit password')
	parser.add_argument('-s, --suppress', dest='supp', action='store_true', required=False,help='Suppress Output')
	parser.add_argument('--version', action='version', version='%(prog)s 0.2')

	return parser.parse_args()



def login(usrname, passwd):
	"""Log in to reddit and save cookie"""

	pass_dict = {'user' : usrname,
				 'passwd' : passwd,
				 'api_type' : 'json',}

	headers = {'user-agent' : 'Nisroc\'s Reddit Bot',}

	client = requests.session()
	client.headers = headers

	try:
		r = client.post('http://www.reddit.com/api/login', data=pass_dict)
		j = json.loads(r.text)
	except ConnectionError:
		sys.exit("Max retries exceeded with url.")

	client.modhash = j['json']['data']['modhash']
	client.user = usrname

	return client



def getTitles(html, imgur=False):
	if imgur == False:
		soup = BeautifulSoup(html)
		sAll = soup.findAll("a", {"class" : "title may-blank loggedin"})
	else:
		soup = BeautifulSoup(html)
		sAll = soup.findAll('a', {'class' : 'zoom'})

	return sAll



def dlImage(url, path):
	try:
		fname = url.split('/')

		if fname[-1].endswith('?1'):
			fname[-1] = fname[-1].strip('?1')

		with open('%s%s' % (path, fname[-1]), 'wb') as file:
			response = requests.get('%s' % url, stream=True)

			if not response.ok:
				print "Could not download image."
				return "failed!"
			for block in response.iter_content(1024):
				if not block:
					break

				file.write(block)
	except KeyboardInterrupt, IOError:
		sys.exit("Could not download image or interupted by user.")



def compareFiles(link, files):
	for file in files:
		if file in link:
			return True
	return False



if __name__ == "__main__":
	base = cmdArgs()

	if not base.r:
		sys.exit("-a attribute (sub name) needed.")

	if base.supp:
		null_out = open(os.devnull, 'w')
		sys.stdout = null_out
		sys.stderr = null_out

	url = "http://www.reddit.com/r/" + base.r

	client = login(base.u, base.p)
	page = client.get(url)

	titles = getTitles(page.text)

	path = os.getcwd() + "/img/" + base.r + "/"
	if not os.path.exists(path):
		os.makedirs(path)

	files = os.listdir(path)

	links = [[item.get('href').encode('utf-8'), item.getText().encode('utf-8')] for item in titles]

	part = 0 # count
	ln = len(links) # length

	for link in links: # link[0] = image url, link[1] = image title
		part += 1
		if compareFiles(link[0], files):
			print("[{:2}/{:2}] File '{}' exists".format(part,ln,link[1]))
		else:
			if '.jpg' in link[0]:
				print("[{:2}/{:2}] Downloading image \"{}\" ... ".format(part,ln, link[1]))
				dlImage(link[0], path)
			elif "http://imgur.com/a/" in link[0]:
				albumHtml = requests.get(link[0])
				album = getTitles(albumHtml.text, True)
				albumImgs = [items.get('href') for items in album]

				print("[{:2}/{:2}] Downloading album \"{}\" ... ".format(part,ln, link[1]))

				aPart = 0
				aLn = len(albumImgs)

				for link in albumImgs:
					aLn += 1
					print("\t[{:2}/{:2}] Downloading album image ... ".format(aPart,aLn))
					dlImage("http:%s" % link, path)
